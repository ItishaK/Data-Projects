{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb89b243-a013-4cde-af37-4b8760d5b5e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schema and First three Rows"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EmpID: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+-----+-----+----------+------+\n|EmpID| Name|Department|Salary|\n+-----+-----+----------+------+\n|    1| Amit|        IT| 60000|\n|    2|Priya|        HR| 55000|\n|    3|Rahul|   Finance| 75000|\n+-----+-----+----------+------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "(1, \"Amit\", \"IT\", 60000),\n",
    "(2, \"Priya\", \"HR\", 55000),\n",
    "(3, \"Rahul\", \"Finance\", 75000),\n",
    "(4, \"Sneha\", \"IT\", 80000),\n",
    "(5, \"Karan\", \"HR\", 65000)\n",
    "]\n",
    "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.describe()\n",
    "df.printSchema()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13473b7c-d149-4e5e-a221-8309ff97fa36",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Employees earning more than 70,000 salary"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+------+\n|EmpID| Name|Department|Salary|\n+-----+-----+----------+------+\n|    3|Rahul|   Finance| 75000|\n|    4|Sneha|        IT| 80000|\n+-----+-----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "filter_df = df.filter(col('Salary') > 70000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b325bbbf-b5b1-4a83-8400-cf42598373af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Average Salary per Department"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n|Department|Avg_Sal|\n+----------+-------+\n|        IT|70000.0|\n|        HR|60000.0|\n|   Finance|75000.0|\n+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "avg_sal_df = df.groupBy(col('Department')).agg(avg(col('Salary')).alias('Avg_Sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86a124e-2333-4f96-8d22-736f6fa698da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Employees whose Name Starts with 'A'"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n|EmpID|Name|Department|Salary|\n+-----+----+----------+------+\n|    1|Amit|        IT| 60000|\n+-----+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "name_df = df.filter(col('Name').startswith('A')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62baa351-52d8-4bd3-97e8-0a27e7681ba8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Number of Employees per Department"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n|Department|Emp_count|\n+----------+---------+\n|        IT|        2|\n|        HR|        2|\n|   Finance|        1|\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "count_emp_df = df.groupBy(col('Department')).agg(count(col('EmpID')).alias('Emp_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9a039a-a123-4653-9c88-c16350fb2139",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "New Column for Tax Deduction (10% of Salary)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+------+-------------+\n|EmpID| Name|Department|Salary|tax_deduction|\n+-----+-----+----------+------+-------------+\n|    1| Amit|        IT| 60000|       6000.0|\n|    2|Priya|        HR| 55000|       5500.0|\n|    3|Rahul|   Finance| 75000|       7500.0|\n|    4|Sneha|        IT| 80000|       8000.0|\n|    5|Karan|        HR| 65000|       6500.0|\n+-----+-----+----------+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "tax_df = df.withColumn('tax_deduction', (0.1 * col('Salary'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d07ba9-1c26-410d-8fe4-8a176e483557",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Salary in descending Order"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+------+\n|EmpID| Name|Department|Salary|\n+-----+-----+----------+------+\n|    4|Sneha|        IT| 80000|\n|    3|Rahul|   Finance| 75000|\n|    5|Karan|        HR| 65000|\n|    1| Amit|        IT| 60000|\n|    2|Priya|        HR| 55000|\n+-----+-----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "final_df = df.sort(desc('Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa7484d-967c-433b-ab6d-11565198562c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Second Highest Salary"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+------+\n|EmpID| Name|Department|Salary|\n+-----+-----+----------+------+\n|    3|Rahul|   Finance| 75000|\n+-----+-----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "spec = Window.orderBy(desc('Salary'))\n",
    "temp_df = df.withColumn('sal_rank',dense_rank().over(spec))\n",
    "final_df = temp_df.filter(col('sal_rank') == 2).select('EmpID', 'Name', 'Department', 'Salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92680647-631a-49de-b5d0-7bc6700f18d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Employees in the HR or IT Department"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+------+\n|EmpID| Name|Department|Salary|\n+-----+-----+----------+------+\n|    1| Amit|        IT| 60000|\n|    2|Priya|        HR| 55000|\n|    4|Sneha|        IT| 80000|\n|    5|Karan|        HR| 65000|\n+-----+-----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df = df.filter(col('Department').isin('HR', 'IT')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e67b37fc-1713-4f61-8b9c-35d4e509f330",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Total Salary Paid by the Company"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|Total_Salary|\n+------------+\n|      335000|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "final_df = df.agg(round(sum(col('Salary')),2).alias('Total_Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d806c16f-97cc-4f15-89db-bf9bff80171f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading CSV File of Cricket Players"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----+-------+\n|        Player|  Country| Runs|Wickets|\n+--------------+---------+-----+-------+\n|   Virat Kohli|    India|12000|      4|\n|  Rohit Sharma|    India|11000|      8|\n|Jasprit Bumrah|    India| 1200|    200|\n|   Steve Smith|Australia| 9500|     20|\n+--------------+---------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "file_path = '/FileStore/tables/players.csv'\n",
    "players_df = spark.read.csv(file_path, header=True, inferSchema = True)\n",
    "players_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46de01ec-d524-450d-9fe1-b208a3c853e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Player with Maximum Runs"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|     Player| Runs|\n+-----------+-----+\n|Virat Kohli|12000|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spec = Window.orderBy(col('Runs').desc())\n",
    "rank_df = players_df.withColumn('rank', dense_rank().over(spec))\n",
    "final_df = rank_df.filter(col('rank') == 1).drop(\"rank\").select('Player', 'Runs').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9e2586-3630-4ec0-8f74-e24caec93643",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Average Runs Scored by Indian Players"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|Avg_Indian_Run_Score|\n+--------------------+\n|             8066.67|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "filter_df = players_df.filter(col('Country') == 'India')\n",
    "final_df = filter_df.agg(round(avg(col('Runs')),2).alias('Avg_Indian_Run_Score')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e4fd8df-ffcd-4632-a26a-8156740278d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Players Who Have Taken More than 50 Wickets"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----+-------+\n|        Player|Country|Runs|Wickets|\n+--------------+-------+----+-------+\n|Jasprit Bumrah|  India|1200|    200|\n+--------------+-------+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df = players_df.filter(col('Wickets') > 50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d5b7bb-4dc5-43ea-9d3a-d43433d05f13",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading JSON File Containing Indian Cities Population"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n|     City|Population|      State|\n+---------+----------+-----------+\n|   Mumbai|  20000000|Maharashtra|\n|    Delhi|  18000000|      Delhi|\n|Bangalore|  12000000|  Karnataka|\n|Hyderabad|  10000000|  Telangana|\n+---------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "file_path = '/FileStore/tables/cities-2.json'\n",
    "cities_df = spark.read.json(file_path)\n",
    "cities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68990832-337d-4290-9bb3-123306cdfae2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cities with a Population Greater than 15 Million"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------+\n|  City|Population|      State|\n+------+----------+-----------+\n|Mumbai|  20000000|Maharashtra|\n| Delhi|  18000000|      Delhi|\n+------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df = cities_df.filter(col('Population') > 15000000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf49802-6499-4614-b6b2-6153910cc9a5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Total Population per State"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n|      State|Population_per_State|\n+-----------+--------------------+\n|  Karnataka|            12000000|\n|      Delhi|            18000000|\n|Maharashtra|            20000000|\n|  Telangana|            10000000|\n+-----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df = cities_df.groupBy(col('State')).agg(sum(col('Population')).alias('Population_per_State')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f3deb5-fc0c-49ca-bb78-18a3841f3002",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "State with the Highest Total Population"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n|      State|Population_per_State|\n+-----------+--------------------+\n|Maharashtra|            20000000|\n+-----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spec1 = Window.partitionBy('State')\n",
    "filter_df = cities_df.withColumn('Population_per_State', sum(col('Population')).over(spec1)).select('State', 'Population_per_State')\n",
    "\n",
    "spec2 = Window.orderBy(desc('Population_per_State'))\n",
    "rank_df = filter_df.withColumn('pop_rank', dense_rank().over(spec2))\n",
    "final_df = rank_df.filter(col('pop_rank') == 1).select('State', 'Population_per_State').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f8619b-aeda-4555-9aa0-a7513fd79424",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Converting the Spark DataFrame to Pandas DataFrame"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmpID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Department</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Amit</td>\n",
       "      <td>IT</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Priya</td>\n",
       "      <td>HR</td>\n",
       "      <td>55000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rahul</td>\n",
       "      <td>Finance</td>\n",
       "      <td>75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sneha</td>\n",
       "      <td>IT</td>\n",
       "      <td>80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karan</td>\n",
       "      <td>HR</td>\n",
       "      <td>65000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EmpID</th>\n      <th>Name</th>\n      <th>Department</th>\n      <th>Salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Amit</td>\n      <td>IT</td>\n      <td>60000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Priya</td>\n      <td>HR</td>\n      <td>55000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Rahul</td>\n      <td>Finance</td>\n      <td>75000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Sneha</td>\n      <td>IT</td>\n      <td>80000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Karan</td>\n      <td>HR</td>\n      <td>65000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas_df = df.toPandas()\n",
    "pandas_df.head()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark task",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
